# -*- coding: utf-8 -*-
"""EFFICIENTNETimagenetLABEL

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZIw4znzpijH5IkBXcAfuTbTDoyivU6FH
"""

# Commented out IPython magic to ensure Python compatibility.
import os

NUM_WORKERS = 20
os.environ['MKL_NUM_THREADS'] = str(NUM_WORKERS)
os.environ['NUMEXPR_NUM_THREADS'] = str(NUM_WORKERS)
os.environ['OMP_NUM_THREADS'] = str(NUM_WORKERS)
os.environ["CUDA_VISIBLE_DEVICES"] = str(0)
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets, transforms, models
from PIL import Image
from tqdm import tqdm
import timm
import numpy as np
from pathlib import Path
from torch.optim import lr_scheduler
import torch
from PIL import Image
import torchvision
import torchvision.transforms as transforms
import numpy as np
import json
import requests
import matplotlib.pyplot as plt
import warnings
from torchvision.models import efficientnet_b4
warnings.filterwarnings('ignore')
# %matplotlib inline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
print(f'Using {device} for inference')
torch.manual_seed(42)
# Set number of workers

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

Path('./sub_folder').mkdir(parents=True, exist_ok=True)

# Set random seed for reproducibility
torch.manual_seed(42)

# Define mean and std for normalization
tiny_imagenet_mean = (0.4802, 0.4481, 0.3975)
tiny_imagenet_std = (0.2302, 0.2265, 0.2262)

# Define data transformations
train_transform = transforms.Compose([
    transforms.RandomRotation(10),
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(64, padding=4, padding_mode="reflect"),
    transforms.ToTensor(),
    transforms.Normalize(tiny_imagenet_mean, tiny_imagenet_std)
])

test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(tiny_imagenet_mean, tiny_imagenet_std)
])

# Path to your Tiny ImageNet data
train_dir = './tiny-imagenet-200/train'
val_dir = './tiny-imagenet-200/val'

# Load datasets using ImageFolder for training data
trainset = datasets.ImageFolder(root=train_dir, transform=train_transform)

# Custom Dataset class for validation data
class TinyImageNetValDataset(Dataset):
    def __init__(self, val_dir, transform=None):
        self.val_dir = val_dir
        self.transform = transform
        self.img_dir = os.path.join(val_dir, 'images')
        self.img_list = os.listdir(self.img_dir)
        self.labels = self._load_labels(os.path.join(val_dir, 'val_annotations.txt'))

    def _load_labels(self, annotations_file):
        labels = {}
        with open(annotations_file, 'r') as f:
            for line in f:
                parts = line.strip().split('\t')
                img_name = parts[0]
                label = parts[1]
                labels[img_name] = label
        return labels

    def __len__(self):
        return len(self.img_list)

    def __getitem__(self, idx):
        img_name = self.img_list[idx]
        img_path = os.path.join(self.img_dir, img_name)
        image = Image.open(img_path).convert('RGB')
        label = self.labels[img_name]
        label_idx = trainset.class_to_idx[label]
        if self.transform:
            image = self.transform(image)
        return image, label_idx

valset = TinyImageNetValDataset(val_dir, transform=test_transform)

# Loop over percentages from 0% to 100% in 10% intervals
target_class = 3  # Class to corrupt
new_label = 5  # New label (arbitrary)
num_epochs = 15
for percentage_to_change in range(0, 101, 10):
    learning_rate = 0.01
    weight_decay = 1e-4
    momentum = 0.9
    batch_size = 128

    # Adjust labels according to corruption percentage
    train_labels = np.array(trainset.targets)
    num_to_change = int((percentage_to_change / 100) * np.sum(train_labels == target_class))
    indices_to_change = np.random.choice(np.where(train_labels == target_class)[0], num_to_change, replace=False)
    train_labels[indices_to_change] = new_label
    trainset.targets = list(train_labels)

    # DataLoader instances
    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=NUM_WORKERS)
    test_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)

    # Initialize EfficientNet V2 B0 model
    model = timm.create_model('tf_efficientnetv2_b0', pretrained=True, num_classes=200)

    # Modify the input layer to be compatible with 64x64 Tiny ImageNet images
    model.conv_stem = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)
    model.bn1 = nn.BatchNorm2d(32)
    model.act1 = nn.SiLU()
    model.classifier = nn.Linear(model.classifier.in_features, 200)
    model = model.to(device)

    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)
    scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)
    criterion = nn.CrossEntropyLoss()

    # Training and validation loop
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0
        correct_predictions = 0
        total_samples = 0

        # Initialize tqdm progress bar for the training data
        train_data_loader = tqdm(train_loader, total=len(train_loader), desc=f'Epoch [{epoch + 1}/{num_epochs}] Training')

        for batch_idx, (inputs, targets) in enumerate(train_data_loader):
            optimizer.zero_grad()
            inputs, targets = inputs.to(device), targets.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            correct_predictions += predicted.eq(targets).sum().item()
            total_samples += targets.size(0)

            train_data_loader.set_postfix(loss=total_loss / (batch_idx + 1), accuracy=100 * correct_predictions / total_samples)

        # Evaluate model on validation dataset after completing an epoch on the training dataset
        model.eval()
        correct_val, total_val, total_loss_val = 0, 0, 0
        with torch.no_grad():
            for (inputs, targets) in test_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                total_loss_val += loss.item()

                _, predicted = outputs.max(1)
                correct_val += predicted.eq(targets).sum().item()
                total_val += targets.size(0)

        accuracy_val = 100 * correct_val / total_val
        average_loss_val = total_loss_val / len(test_loader)

        print(f'Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {total_loss / len(train_loader):.4f}, Train Accuracy: {100 * correct_predictions / total_samples:.2f}%, Val Loss: {average_loss_val:.4f}, Val Accuracy: {accuracy_val:.2f} %')

        # Step the learning rate scheduler
        if scheduler:
            scheduler.step()

    # Save model checkpoint after all epochs are completed for each corruption percentage
    model_save_path = f'./sub_folder/efficientnetv2_pct_{percentage_to_change}.pth'
    torch.save({
        'epoch': num_epochs,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'train_loss': total_loss / len(train_loader),
        'train_accuracy': correct_predictions / total_samples,
        'val_loss': average_loss_val,
        'val_accuracy': accuracy_val
    }, model_save_path)

    print(f"Training completed for {percentage_to_change}% label corruption and model saved.")

print("All training completed.")
