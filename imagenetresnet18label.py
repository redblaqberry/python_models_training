# -*- coding: utf-8 -*-
"""imagenetresnet18label

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZIw4znzpijH5IkBXcAfuTbTDoyivU6FH
"""

import os
# Set number of workers
NUM_WORKERS = 20
os.environ['MKL_NUM_THREADS'] = str(NUM_WORKERS)
os.environ['NUMEXPR_NUM_THREADS'] = str(NUM_WORKERS)
os.environ['OMP_NUM_THREADS'] = str(NUM_WORKERS)
os.environ["CUDA_VISIBLE_DEVICES"] = str(0)
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
import torchvision
from tqdm import tqdm
from pathlib import Path
import numpy as np
from PIL import Image
from torchvision import datasets, transforms, models


# Create directories
Path('./sub_folder').mkdir(parents=True, exist_ok=True)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f'Using {device} for inference')
torch.manual_seed(42)

# Tiny ImageNet dataset normalization
tiny_imagenet_mean = (0.4802, 0.4481, 0.3975)
tiny_imagenet_std = (0.2302, 0.2265, 0.2262)

# Normalization
normalize = transforms.Normalize(mean=tiny_imagenet_mean, std=tiny_imagenet_std)

# Data transformations
transform_train = transforms.Compose([
    transforms.RandomRotation(10),
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(64, padding=4, padding_mode="reflect"),
    transforms.ToTensor(),
    normalize
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    normalize
])

# Path to your Tiny ImageNet data
train_dir = './tiny-imagenet-200/train'
val_dir = './tiny-imagenet-200/val'

# Load datasets using ImageFolder for training data
trainset = datasets.ImageFolder(root=train_dir, transform=transform_train)

# Custom Dataset class for validation data
class TinyImageNetValDataset(Dataset):
    def __init__(self, val_dir, transform=None):
        self.val_dir = val_dir
        self.transform = transform
        self.img_dir = os.path.join(val_dir, 'images')
        self.img_list = os.listdir(self.img_dir)
        self.labels = self._load_labels(os.path.join(val_dir, 'val_annotations.txt'))

    def _load_labels(self, annotations_file):
        labels = {}
        with open(annotations_file, 'r') as f:
            for line in f:
                parts = line.strip().split('\t')
                img_name = parts[0]
                label = parts[1]
                labels[img_name] = label
        return labels

    def __len__(self):
        return len(self.img_list)

    def __getitem__(self, idx):
        img_name = self.img_list[idx]
        img_path = os.path.join(self.img_dir, img_name)
        image = Image.open(img_path).convert('RGB')
        label = self.labels[img_name]
        label_idx = trainset.class_to_idx[label]
        if self.transform:
            image = self.transform(image)
        return image, label_idx

valset = TinyImageNetValDataset(val_dir, transform=transform_test)

# Prepare fixed parameters
target_class = 3  # Class to corrupt
new_label = 5  # New label (arbitrary)
num_epochs = 25

# Loop over percentages from 0% to 100% in 10% intervals
for percentage_to_change in range(0, 101, 10):
    learning_rate = 0.01
    weight_decay = 1e-4
    momentum = 0.9
    batch_size = 64

    # Adjust labels according to corruption percentage
    train_labels = np.array(trainset.targets)
    num_to_change = int((percentage_to_change / 100) * np.sum(train_labels == target_class))
    indices_to_change = np.random.choice(np.where(train_labels == target_class)[0], num_to_change, replace=False)
    train_labels[indices_to_change] = new_label
    trainset.targets = list(train_labels)

    # DataLoader instances
    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=NUM_WORKERS)
    testloader = DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)

    # Initialize ResNet18 model
    model = torchvision.models.resnet18(pretrained=True)
    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
    model.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
    model.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    model.fc = nn.Linear(512, 200)
    model = model.to(device)

    # Initialize optimizer, scheduler, and criterion
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)
    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=4, threshold=0.001, eps=1e-8)
    criterion = nn.CrossEntropyLoss()

    # Training loop
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0
        correct_predictions = 0
        total_samples = 0

        data_loader = tqdm(trainloader, total=len(trainloader), desc=f'Epoch [{epoch + 1}/{num_epochs}]')

        for batch_idx, (inputs, targets) in enumerate(data_loader):
            optimizer.zero_grad()
            inputs, targets = inputs.to(device), targets.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            correct_predictions += predicted.eq(targets).sum().item()
            total_samples += targets.size(0)

            data_loader.set_postfix(loss=total_loss / (batch_idx + 1), accuracy=correct_predictions / total_samples)

        # Evaluation
        model.eval()
        correct_test, total_test, total_loss_test = 0, 0, 0
        with torch.no_grad():
            for (inputs, targets) in testloader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                total_loss_test += loss.item()
                _, predicted = outputs.max(1)
                correct_test += predicted.eq(targets).sum().item()
                total_test += targets.size(0)

        accuracy_test = correct_test / total_test
        average_loss_test = total_loss_test / len(testloader)

        data_loader.set_postfix(train_loss=total_loss / len(trainloader), train_accuracy=correct_predictions / total_samples,
                                test_loss=average_loss_test, test_accuracy=accuracy_test)

        scheduler.step(average_loss_test)

        print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {total_loss / len(trainloader):.4f}, '
              f'Accuracy: {correct_predictions / total_samples * 100:.2f}%, '
              f'Loss on test data: {average_loss_test:.4f}, '
              f'Accuracy on test data: {accuracy_test * 100:.2f}%')

    # Save model state and training stats
    training_stats = {
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'loss': total_loss / len(trainloader),
        'accuracy': correct_predictions / total_samples,
        'test_accuracy': accuracy_test,
        'test_loss': average_loss_test,
        'learning_rate': optimizer.param_groups[0]["lr"]
    }

    torch.save(training_stats, f"./sub_folder/tinyimagenet_label_resnet18_{percentage_to_change}.pth")

print("All training completed.")
